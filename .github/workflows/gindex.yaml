name: Check Google Indexing Status
on: 
  workflow_dispatch:
  schedule:
    - cron: '0 0 1 * *'  # Run on the first day of each month

permissions:
  contents: write

jobs:
  check_indexing:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests beautifulsoup4 pandas

      - name: Check Google Indexing
        id: google_index
        env:
          REPO_NAME: "${{ github.event.repository.name }}"
        run: |
          python << EOF
          import requests
          from bs4 import BeautifulSoup
          import pandas as pd
          import urllib.parse
          import re
          import os
          from datetime import datetime

          # Convert repository name to lowercase
          repo_name = os.environ['REPO_NAME'].lower()
          site_url = f"https://{repo_name}"

          # Function to get Google search results
          def get_google_results(query, start=0):
              params = {
                  'q': query,
                  'num': 100,
                  'start': start
              }
              headers = {
                  'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
              }
              response = requests.get('https://www.google.com/search', params=params, headers=headers)
              return response.text

          # Parse total indexed pages
          def parse_total_results(html):
              soup = BeautifulSoup(html, 'html.parser')
              result_stats = soup.find('div', id='result-stats')
              if result_stats:
                  match = re.search(r'about\s+([\d,]+)', result_stats.text)
                  return match.group(1) if match else 'N/A'
              return 'N/A'

          # Detect available search result pages
          def detect_search_pages(html):
              soup = BeautifulSoup(html, 'html.parser')
              page_links = soup.find_all('td', class_='NKTSme')
              pages = []
              for link in page_links:
                  a_tag = link.find('a')
                  if a_tag and 'start' in a_tag.get('href', ''):
                      start_param = re.search(r'start=(\d+)', a_tag['href'])
                      if start_param:
                          pages.append(int(start_param.group(1)))
              return sorted(set(pages))

          # Extract Google URLs
          def extract_google_urls(html):
              soup = BeautifulSoup(html, 'html.parser')
              urls = []
              for link in soup.find_all('div', class_='yuRUbf'):
                  a_tag = link.find('a')
                  if a_tag:
                      original_url = a_tag['href']
                      google_redirect_url = f"https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&url={urllib.parse.quote(original_url)}"
                      urls.append({
                          'original_url': original_url,
                          'google_redirect_url': google_redirect_url
                      })
              return urls

          # Main scraping logic
          site_query = f"site:{site_url}"
          all_results = []
          
          # Get first page results
          first_page_html = get_google_results(site_query)
          total_results = parse_total_results(first_page_html)
          
          # Detect available search result pages
          search_pages = detect_search_pages(first_page_html)
          
          # Collect URLs from first page
          first_page_urls = extract_google_urls(first_page_html)
          all_results.extend(first_page_urls)

          # Collect URLs from additional pages
          for start in search_pages[1:]:  # Skip the first page as we've already processed it
              page_html = get_google_results(site_query, start)
              page_urls = extract_google_urls(page_html)
              all_results.extend(page_urls)

          # Create DataFrame
          df = pd.DataFrame(all_results)

          # Generate monthly filename
          current_month = datetime.now().strftime("%Y-%m")
          filename = f'serp-status-{current_month}.md'

          # Write results to Markdown
          with open(filename, 'w') as f:
              f.write(f"# Google Indexing Status for {site_url}\n\n")
              f.write(f"## Measurement Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
              f.write(f"## Total Indexed Pages: {total_results}\n\n")
              f.write("| Original URL | Google Redirect URL |\n")
              f.write("|--------------|---------------------|\n")
              for _, row in df.iterrows():
                  f.write(f"| {row['original_url']} | {row['google_redirect_url']} |\n")

          print(f"Total Indexed Pages: {total_results}")
          print(f"Number of URLs found: {len(all_results)}")
          EOF

      - name: Commit and Push Results
        env:
          GH_PAT: ${{ secrets.GH_PAT }}
        run: |
          git config --global user.name 'github-actions[bot]'
          git config --global user.email 'github-actions[bot]@users.noreply.github.com'
          git add serp-status-*.md
          git commit -m 'Update Monthly Google Indexing Status' || echo "No changes to commit"
          git push https://x-access-token:${{ secrets.GH_PAT }}@github.com/${{ github.repository }} HEAD:main
