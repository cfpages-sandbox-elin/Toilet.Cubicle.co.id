name: Google Index Status v2.1
on: 
  workflow_dispatch:
  schedule:
    - cron: '0 0 1 * *'  # Run on the first day of each month

permissions:
  contents: write

jobs:
  check_indexing:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests beautifulsoup4 pandas

      - name: Check Google Indexing
        id: google_index
        env:
          REPO_NAME: "${{ github.event.repository.name }}"
        run: |
          python << EOF
          import requests
          from bs4 import BeautifulSoup
          import pandas as pd
          import urllib.parse
          import re
          import os
          from datetime import datetime

          def get_google_results(query, num=100, start=0, max_retries=3, retry_delay=5):
              headers = {
                  'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.6478.114 Safari/537.36',
                  'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
                  'Accept-Language': 'en-US,en;q=0.9',
                  'Accept-Encoding': 'gzip, deflate, br',
                  'Referer': 'https://www.google.com/',
                  'DNT': '1',
                  'Connection': 'keep-alive',
                  'Upgrade-Insecure-Requests': '1',
                  'Sec-Fetch-Dest': 'document',
                  'Sec-Fetch-Mode': 'navigate',
                  'Sec-Fetch-Site': 'same-origin',
                  'Sec-Fetch-User': '?1',
                  'Sec-Ch-Ua': '"Chromium";v="126", "Google Chrome";v="126", "Not.A/Brand";v="24"',
                  'Sec-Ch-Ua-Mobile': '?0',
                  'Sec-Ch-Ua-Platform': '"Windows"'
              }
              
              params = {
                  'q': query,
                  'num': num,
                  'start': start,
                  'client': 'chrome',
                  'oq': query,
                  'sclient': 'gws-wiz-serp',
                  'sourceid': 'chrome',
                  'ie': 'UTF-8',
                  'biw': '1263',
                  'bih': '397',
                  'uact': '5'
              }
              
              retries = 0
              while retries < max_retries:
                  try:
                      # Construct and print the full URL for debugging
                      full_url = requests.Request('GET', 'https://www.google.com/search', params=params).prepare().url
                      response = requests.get('https://www.google.com/search', params=params, headers=headers)
                      
                      # Print details for debug
                      print(f"Requesting URL: {full_url}")
                      print(f"Parameters: {params}")
                      print(f"Response Status Code: {response.status_code}")
                      print(f"Response Headers: {response.headers}")
                      
                      # Save raw HTML for inspection if needed
                      with open('google_response.html', 'w', encoding='utf-8') as f:
                          f.write(response.text)
                      
                      if response.status_code == 429:  # Too Many Requests
                          retries += 1
                          print(f"Rate limit hit, retrying in {retry_delay} seconds...")
                          import time
                          time.sleep(retry_delay)
                          continue
                      
                      response.raise_for_status()
                      return response.text
                  
                  except requests.exceptions.RequestException as e:
                      print(f"Error fetching Google results: {e}")
                      retries += 1
                      if retries >= max_retries:
                          print(f"Failed to fetch Google results after {max_retries} retries")
                          return None
                      print(f"Retrying in {retry_delay} seconds...")
                      time.sleep(retry_delay)
              
              return None

          def parse_total_results(html):
              soup = BeautifulSoup(html, 'html.parser')
              result_stats = soup.find('div', id='result-stats')
              if result_stats:
                  match = re.search(r'About\s+([\d,]+)', result_stats.text)
                  return int(match.group(1).replace(',', '')) if match else 'N/A'
              return 'N/A'

          def detect_search_pages(html):
              soup = BeautifulSoup(html, 'html.parser')
              page_links = soup.find_all('td', class_='NKTSme')
              pages = []
              for link in page_links:
                  a_tag = link.find('a')
                  if a_tag and 'start' in a_tag.get('href', ''):
                      start_param = re.search(r'start=(\d+)', a_tag['href'])
                      if start_param:
                          pages.append(int(start_param.group(1)))
              return sorted(set(pages))

          def extract_google_urls(html, debug_file_path=None):
              soup = BeautifulSoup(html, 'html.parser')
              
              # Optional: Save debug HTML if debug file path is provided
              if debug_file_path:
                  with open(debug_file_path, 'w', encoding='utf-8') as f:
                      f.write(html)
              
              # Print some debugging information about the parsed HTML
              print("Parsing HTML...")
              print(f"HTML length: {len(html)} characters")
              
              # Check if the HTML contains any expected Google search content
              title = soup.find('title')
              print(f"Page Title: {title.text if title else 'No title found'}")
              
              # Look for search result containers
              result_containers = soup.find_all('div', class_='yuRUbf')
              print(f"Number of result containers found: {len(result_containers)}")
              
              urls = []
              for link in result_containers:
                  a_tag = link.find('a')
                  if a_tag and 'href' in a_tag.attrs:
                      original_url = a_tag['href']
                      print(f"Found URL: {original_url}")
                      
                      # Try multiple methods to get attributes
                      ved = a_tag.get('data-ved') or ''
                      usg = a_tag.get('data-usg') or ''
                      
                      print(f"VED attribute: {ved}")
                      print(f"USG attribute: {usg}")
                      
                      # Construct redirect URL (similar to previous version)
                      redirect_url = (
                          f"https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&"
                          f"ved={urllib.parse.quote(ved)}&"
                          f"url={urllib.parse.quote(original_url)}&"
                          f"usg={urllib.parse.quote(usg)}"
                      )
                      
                      urls.append({
                          'original_url': original_url,
                          'google_redirect_url': redirect_url
                      })
              
              return urls

          def sanitize_filename(filename):
              """
              Remove or replace characters that are not allowed in filenames
              """
              # Replace forward slashes and other problematic characters
              return re.sub(r'[/\\?%*:|"<>]', '_', str(filename))
          
          def main():
              repo_name = os.environ.get('REPO_NAME').lower()
              site_url = f"https://{repo_name}"

              site_query = f"site:{site_url}"
              all_results = []
              total_results = 'NA'  # Changed from 'N/A' to avoid filename issues

              # Ensure .md and .debug directories exist
              os.makedirs('.md', exist_ok=True)
              os.makedirs('.debug', exist_ok=True)

              # Get current timestamp
              current_timestamp = datetime.now().strftime("%Y%m%d-%H%M")

              # Get first page results with num=10 to get the total indexed pages
              first_page_html = get_google_results(site_query, num=10)
              if first_page_html:
                  total_results = parse_total_results(first_page_html)
                  
                  # Detect available search result pages
                  search_pages = detect_search_pages(first_page_html)
                  
                  # Collect URLs from all pages
                  for start in search_pages:
                      # Generate debug file path for this page
                      debug_filename = f'.debug/serp-status-debug-{current_timestamp}-page{start}.html'
                      
                      # Get results for this page
                      page_html = get_google_results(site_query, num=100, start=start)
                      if page_html:
                          page_urls = extract_google_urls(page_html, debug_file_path=debug_filename)
                          all_results.extend(page_urls)

              # Create DataFrame
              df = pd.DataFrame(all_results)

              # Sanitize filename components
              sanitized_total_results = sanitize_filename(total_results)
              sanitized_url_count = sanitize_filename(len(all_results))

              # Generate output file path with more descriptive name
              filename = f'.md/serp-status-{current_timestamp}-{sanitized_total_results}-{sanitized_url_count}.md'

              # Write results to Markdown
              with open(filename, 'w') as f:
                  f.write(f"# Google Indexing Status for {site_url}\n\n")
                  f.write(f"## Measurement Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
                  f.write(f"## Total Indexed Pages: {total_results}\n\n")
                  
                  # Add debug file links
                  f.write("## Debug Information\n")
                  debug_files = sorted([f for f in os.listdir('.debug') if f.startswith('serp-status-debug')])
                  for debug_file in debug_files:
                      f.write(f"- [Debug Page HTML](.debug/{debug_file})\n")
                  
                  f.write("\n## URL Details\n")
                  f.write("| No. | Original URL | Google Redirect URL |\n")
                  f.write("|-----|--------------|---------------------|\n")
                  for i, (_, row) in enumerate(df.iterrows(), start=1):
                      f.write(f"| {i} | {row['original_url']} | {row['google_redirect_url']} |\n")

              print(f"Total Indexed Pages: {total_results}")
              print(f"Number of URLs found: {len(all_results)}")
              print(f"Results saved to: {filename}")

          if __name__ == "__main__":
              main()
          EOF

      - name: Commit and Push Results
        env:
          GH_PAT: ${{ secrets.GH_PAT }}
        run: |
          git config --global user.name 'github-actions[bot]'
          git config --global user.email 'github-actions[bot]@users.noreply.github.com'
          git add .md/serp-status-*.md
          git commit -m 'Update Monthly Google Indexing Status' || echo "No changes to commit"
          git push https://x-access-token:${{ secrets.GH_PAT }}@github.com/${{ github.repository }} HEAD:main
