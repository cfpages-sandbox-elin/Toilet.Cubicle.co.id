name: Google Index Status v3.2selenium
on: 
  workflow_dispatch:
  schedule:
    - cron: '0 0 1 * *'  # Run on the first day of each month

permissions:
  contents: write

jobs:
  check_indexing:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests beautifulsoup4 pandas

      - name: Check Google Indexing
        id: google_index
        env:
          REPO_NAME: "${{ github.event.repository.name }}"
        run: |
          python << EOF
          import requests
          import os
          import re
          import time
          import urllib.parse
          from datetime import datetime
          import pandas as pd
          from selenium import webdriver
          from selenium.webdriver.chrome.service import Service
          from selenium.webdriver.chrome.options import Options
          from selenium.webdriver.common.by import By
          from selenium.webdriver.support.ui import WebDriverWait
          from selenium.webdriver.support import expected_conditions as EC
          from webdriver_manager.chrome import ChromeDriverManager

          def setup_selenium_driver():
              """Set up Chrome WebDriver with appropriate options."""
              chrome_options = Options()
              chrome_options.add_argument('--headless')
              chrome_options.add_argument('--no-sandbox')
              chrome_options.add_argument('--disable-dev-shm-usage')
              chrome_options.add_argument('--disable-gpu')
              chrome_options.add_argument('--remote-debugging-port=9222')
              
              service = Service(ChromeDriverManager().install())
              driver = webdriver.Chrome(service=service, options=chrome_options)
              return driver

          def get_google_results(query, domain='com', max_retries=3):
              """Fetch Google search results using Selenium."""
              for attempt in range(max_retries):
                  driver = None
                  try:
                      driver = setup_selenium_driver()
                      full_url = f'https://www.google.{domain}/search?q={urllib.parse.quote(query)}'
                      
                      driver.get(full_url)
                      
                      # Wait for results to load
                      WebDriverWait(driver, 10).until(
                          EC.presence_of_element_located((By.CSS_SELECTOR, 'div.yuRUbf'))
                      )
                      
                      # Extract results
                      result_links = driver.find_elements(By.CSS_SELECTOR, 'div.yuRUbf a')
                      
                      urls = []
                      for link in result_links:
                          original_url = link.get_attribute('href')
                          ved = link.get_attribute('data-ved') or ''
                          usg = link.get_attribute('data-usg') or ''
                          
                          # Construct redirect URL
                          google_redirect_url = (
                              f"https://www.google.{domain}/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&"
                              f"ved={urllib.parse.quote(ved)}&"
                              f"url={urllib.parse.quote(original_url)}&"
                              f"usg={urllib.parse.quote(usg)}"
                          ) if ved and usg else original_url
                          
                          urls.append({
                              'original_url': original_url, 
                              'google_redirect_url': google_redirect_url,
                              'ved': ved,
                              'usg': usg,
                              'domain': domain
                          })
                      
                      # Try to get total results count
                      try:
                          result_stats = driver.find_element(By.ID, 'result-stats')
                          total_results_match = re.search(r'About\s+([\d,]+)', result_stats.text)
                          total_results = int(total_results_match.group(1).replace(',', '')) if total_results_match else 0
                      except Exception:
                          total_results = 0
                      
                      return urls, total_results
                  
                  except Exception as e:
                      print(f"Attempt {attempt + 1} failed: {e}")
                      time.sleep(5)  # Wait before retrying
                  
                  finally:
                      if driver:
                          driver.quit()
              
              return [], 0

          def merge_url_results(results_com, results_co_id):
              """Merge and deduplicate results from different domains."""
              url_dict = {}
              
              for result in results_com + results_co_id:
                  url = result['original_url']
                  
                  if (url not in url_dict or 
                      (not url_dict[url]['ved'] and result['ved']) or 
                      (not url_dict[url]['usg'] and result['usg'])):
                      url_dict[url] = result
              
              return list(url_dict.values())

          def parse_total_results(html):
              soup = BeautifulSoup(html, 'html.parser')
              result_stats = soup.find('div', id='result-stats')
              if result_stats:
                  match = re.search(r'About\s+([\d,]+)', result_stats.text)
                  if match:
                      return int(match.group(1).replace(',', ''))
              # Fallback logic
              print("Failed to parse total results.")
              return 'N/A'

          def detect_search_pages(html):
              soup = BeautifulSoup(html, 'html.parser')
              page_links = soup.find_all('td', class_='NKTSme')
              pages = []
              for link in page_links:
                  a_tag = link.find('a')
                  if a_tag and 'start' in a_tag.get('href', ''):
                      start_param = re.search(r'start=(\d+)', a_tag['href'])
                      if start_param:
                          pages.append(int(start_param.group(1)))
              return sorted(set(pages))

          def extract_google_urls(html, domain='com', debug_file_path=None):
              os.makedirs('.debug', exist_ok=True)
              
              # If no specific debug file path is provided, generate one
              if debug_file_path is None:
                  current_timestamp = datetime.now().strftime("%Y%m%d-%H%M")
                  debug_file_path = f'.debug/serp-status-debug-extract-{current_timestamp}-{domain}.html'
              
              save_debug_html(html, debug_file_path)
              
              soup = BeautifulSoup(html, 'html.parser')
              result_containers = soup.find_all('div', class_='yuRUbf')
              urls = []
              for link in result_containers:
                  a_tag = link.find('a')
                  if a_tag and 'href' in a_tag.attrs:
                      original_url = a_tag['href']
                      ved = a_tag.get('data-ved', '')
                      usg = a_tag.get('data-usg', '')
                      
                      # Construct redirect URL if possible
                      google_redirect_url = (
                          f"https://www.google.{domain}/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&"
                          f"ved={urllib.parse.quote(ved)}&"
                          f"url={urllib.parse.quote(original_url)}&"
                          f"usg={urllib.parse.quote(usg)}"
                      ) if ved and usg else original_url
                      
                      urls.append({
                          'original_url': original_url, 
                          'google_redirect_url': google_redirect_url,
                          'ved': ved,
                          'usg': usg,
                          'domain': domain
                      })
              return urls

          def sanitize_filename(filename):
              """Sanitize filename to remove problematic characters."""
              return re.sub(r'[/\\?%*:|"<>]', '_', str(filename))

          def save_debug_html(html, debug_file_path=None):
              if debug_file_path is None:
                  # Generate a default debug file path if none is provided
                  current_timestamp = datetime.now().strftime("%Y%m%d-%H%M")
                  debug_file_path = f'.debug/serp-status-debug-{current_timestamp}.html'
              
              os.makedirs(os.path.dirname(debug_file_path), exist_ok=True)
              try:
                  with open(debug_file_path, 'w', encoding='utf-8') as f:
                      f.write(html)
                  print(f"Debug file saved: {debug_file_path}")
              except Exception as e:
                  print(f"Error saving debug file: {e}")

          def main():
              repo_name = os.environ.get('REPO_NAME', '').lower()
              site_url = f"{repo_name}"
              site_query = f"site:{site_url}"

              # Ensure directories exist
              os.makedirs('.md', exist_ok=True)
              os.makedirs('.debug', exist_ok=True)

              # Get current timestamp
              current_timestamp = datetime.now().strftime("%Y%m%d-%H%M")

              # Collect results from both domains
              all_results = []
              total_results = 0

              # Search on both .com and .co.id domains
              for domain in ['com', 'co.id']:
                  domain_results, domain_total = get_google_results(site_query, domain=domain)
                  all_results.extend(domain_results)
                  total_results = max(total_results, domain_total)

              # Merge and deduplicate results
              merged_results = merge_url_results(
                  [r for r in all_results if r['domain'] == 'com'],
                  [r for r in all_results if r['domain'] == 'co.id']
              )

              # Create DataFrame
              df = pd.DataFrame(merged_results)

              # Sanitize filename components
              sanitized_total_results = sanitize_filename(total_results)
              sanitized_url_count = sanitize_filename(len(merged_results))

              # Generate output file path
              filename = f'.md/serp-status-{current_timestamp}-total{sanitized_total_results}-urls{sanitized_url_count}.md'

              # Write results to Markdown
              with open(filename, 'w') as f:
                  f.write(f"# Google Indexing Status for {site_url}\n\n")
                  f.write(f"## Measurement Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
                  f.write(f"## Total Indexed Pages: {total_results}\n\n")
                  
                  # Add debug file links
                  f.write("## Debug Information\n")
                  for i, debug_file in enumerate(debug_files, start=1):
                      f.write(f"- [Debug Page HTML Page {i}]({debug_file})\n")
                  
                  f.write("\n## URL Details\n")
                  f.write("| No. | Original URL | Google Redirect URL | Domain | VED | USG |\n")
                  f.write("|-----|--------------|---------------------|--------|-----|-----|\n")
                  for i, (_, row) in enumerate(df.iterrows(), start=1):
                      f.write(f"| {i} | {row['original_url']} | {row['google_redirect_url']} | {row['domain']} | {row['ved']} | {row['usg']} |\n")

              print(f"Total Indexed Pages: {total_results}")
              print(f"Number of URLs found: {len(merged_results)}")
              print(f"Results saved to: {filename}")

          if __name__ == "__main__":
              main()
          EOF

      - name: Commit and Push Results
        env:
          GH_PAT: ${{ secrets.GH_PAT }}
        run: |
          git config --global user.name 'github-actions[bot]'
          git config --global user.email 'github-actions[bot]@users.noreply.github.com'
          git add .md/serp-status-*.md
          git add .debug
          git commit -m 'Update Monthly Google Indexing Status' || echo "No changes to commit"
          git push https://x-access-token:${{ secrets.GH_PAT }}@github.com/${{ github.repository }} HEAD:main