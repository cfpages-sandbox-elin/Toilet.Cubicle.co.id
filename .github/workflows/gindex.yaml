name: Google Index Status v3.0
on: 
  workflow_dispatch:
  schedule:
    - cron: '0 0 1 * *'  # Run on the first day of each month

permissions:
  contents: write

jobs:
  check_indexing:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests beautifulsoup4 pandas

      - name: Check Google Indexing
        id: google_index
        env:
          REPO_NAME: "${{ github.event.repository.name }}"
        run: |
          python << EOF
          import requests
          from bs4 import BeautifulSoup
          import pandas as pd
          import urllib.parse
          import re
          import os
          from datetime import datetime

          def get_google_results(query, domain='com', num=100, start=0, max_retries=3, retry_delay=5):
              headers = {
                  'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.6478.114 Safari/537.36'
              }
              
              params = {
                  'q': query,
                  'num': num,
                  'start': start,
                  'client': 'chrome',
                  'oq': query,
                  'sclient': 'gws-wiz-serp',
                  'sourceid': 'chrome',
                  'ie': 'UTF-8',
                  'biw': '1263',
                  'bih': '397',
                  'uact': '5'
              }
              
              retries = 0
              while retries < max_retries:
                  try:
                      # Use the specified domain
                      url = f'https://www.google.{domain}/search'
                      full_url = requests.Request('GET', url, params=params).prepare().url
                      
                      response = requests.get(url, params=params, headers=headers)
                      
                      # Print details for debug
                      print(f"Requesting URL: {full_url}")
                      print(f"Parameters: {params}")
                      print(f"Domain: {domain}")
                      print(f"Response Status Code: {response.status_code}")
                      print(f"Response Headers: {response.headers}")
                      
                      # Save raw HTML for inspection if needed
                      with open('google_response.html', 'w', encoding='utf-8') as f:
                          f.write(response.text)
                      
                      if response.status_code == 429:  # Too Many Requests
                          retries += 1
                          print(f"Rate limit hit, retrying in {retry_delay} seconds...")
                          import time
                          time.sleep(retry_delay)
                          continue
                      
                      response.raise_for_status()
                      return response.text
                  
                  except requests.exceptions.RequestException as e:
                      print(f"Error fetching Google results from {domain}: {e}")
                      retries += 1
                      if retries >= max_retries:
                          print(f"Failed to fetch Google results from {domain} after {max_retries} retries")
                          return None
                      print(f"Retrying in {retry_delay} seconds...")
                      time.sleep(retry_delay)
              
              return None

          def merge_url_results(results_com, results_co_id):
              # Create a dictionary to track unique URLs
              url_dict = {}
              
              # Combine and prioritize results
              for result in results_com + results_co_id:
                  url = result['original_url']
                  
                  # If URL not in dict or this result has more metadata
                  if (url not in url_dict or 
                      (not url_dict[url]['ved'] and result['ved']) or 
                      (not url_dict[url]['usg'] and result['usg'])):
                      url_dict[url] = result
              
              return list(url_dict.values())

          def parse_total_results(html):
              soup = BeautifulSoup(html, 'html.parser')
              result_stats = soup.find('div', id='result-stats')
              if result_stats:
                  match = re.search(r'About\s+([\d,]+)', result_stats.text)
                  if match:
                      return int(match.group(1).replace(',', ''))
              # Fallback logic
              print("Failed to parse total results.")
              return 'N/A'

          def detect_search_pages(html):
              soup = BeautifulSoup(html, 'html.parser')
              page_links = soup.find_all('td', class_='NKTSme')
              pages = []
              for link in page_links:
                  a_tag = link.find('a')
                  if a_tag and 'start' in a_tag.get('href', ''):
                      start_param = re.search(r'start=(\d+)', a_tag['href'])
                      if start_param:
                          pages.append(int(start_param.group(1)))
              return sorted(set(pages))

          def extract_google_urls(html, domain='com', debug_file_path=None):
              os.makedirs('.debug', exist_ok=True)
              
              # If no specific debug file path is provided, generate one
              if debug_file_path is None:
                  current_timestamp = datetime.now().strftime("%Y%m%d-%H%M")
                  debug_file_path = f'.debug/serp-status-debug-extract-{current_timestamp}-{domain}.html'
              
              save_debug_html(html, debug_file_path)
              
              soup = BeautifulSoup(html, 'html.parser')
              result_containers = soup.find_all('div', class_='yuRUbf')
              urls = []
              for link in result_containers:
                  a_tag = link.find('a')
                  if a_tag and 'href' in a_tag.attrs:
                      original_url = a_tag['href']
                      ved = a_tag.get('data-ved', '')
                      usg = a_tag.get('data-usg', '')
                      
                      # Construct redirect URL if possible
                      google_redirect_url = (
                          f"https://www.google.{domain}/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&"
                          f"ved={urllib.parse.quote(ved)}&"
                          f"url={urllib.parse.quote(original_url)}&"
                          f"usg={urllib.parse.quote(usg)}"
                      ) if ved and usg else original_url
                      
                      urls.append({
                          'original_url': original_url, 
                          'google_redirect_url': google_redirect_url,
                          'ved': ved,
                          'usg': usg,
                          'domain': domain
                      })
              return urls

          def sanitize_filename(filename):
              # Replace forward slashes and other problematic characters
              return re.sub(r'[/\\?%*:|"<>]', '_', str(filename))

          def save_debug_html(html, debug_file_path=None):
              if debug_file_path is None:
                  # Generate a default debug file path if none is provided
                  current_timestamp = datetime.now().strftime("%Y%m%d-%H%M")
                  debug_file_path = f'.debug/serp-status-debug-{current_timestamp}.html'
              
              os.makedirs(os.path.dirname(debug_file_path), exist_ok=True)
              try:
                  with open(debug_file_path, 'w', encoding='utf-8') as f:
                      f.write(html)
                  print(f"Debug file saved: {debug_file_path}")
              except Exception as e:
                  print(f"Error saving debug file: {e}")

          def main():
              repo_name = os.environ.get('REPO_NAME', '').lower()
              site_url = f"{repo_name}"
              site_query = f"site:{site_url}"

              # Ensure .md and .debug directories exist
              os.makedirs('.md', exist_ok=True)
              os.makedirs('.debug', exist_ok=True)

              # Get current timestamp
              current_timestamp = datetime.now().strftime("%Y%m%d-%H%M")

              # Collect results from both domains
              all_results = []
              total_results = 'NA'
              debug_files = []

              # Search on both .com and .co.id
              for domain in ['com', 'co.id']:
                  # Get first page results
                  first_page_html = get_google_results(site_query, domain=domain, num=10)
                  
                  if first_page_html:
                      # Save first page debug file
                      first_page_debug_filename = f'.debug/serp-status-debug-{current_timestamp}-{domain}-page1.html'
                      save_debug_html(first_page_html, first_page_debug_filename)
                      debug_files.append(first_page_debug_filename)

                      # Update total results (take the larger value)
                      page_total_results = parse_total_results(first_page_html)
                      total_results = max(total_results, page_total_results)

                      # Extract URLs from first page
                      page_urls = extract_google_urls(first_page_html, domain=domain)
                      all_results.extend(page_urls)

              # Merge and deduplicate results
              merged_results = merge_url_results(
                  [r for r in all_results if r['domain'] == 'com'],
                  [r for r in all_results if r['domain'] == 'co.id']
              )

              # Create DataFrame
              df = pd.DataFrame(merged_results)

              # Sanitize filename components
              sanitized_total_results = sanitize_filename(total_results)
              sanitized_url_count = sanitize_filename(len(merged_results))

              # Generate output file path
              filename = f'.md/serp-status-{current_timestamp}-total{sanitized_total_results}-urls{sanitized_url_count}.md'

              # Write results to Markdown
              with open(filename, 'w') as f:
                  f.write(f"# Google Indexing Status for {site_url}\n\n")
                  f.write(f"## Measurement Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
                  f.write(f"## Total Indexed Pages: {total_results}\n\n")
                  
                  # Add debug file links
                  f.write("## Debug Information\n")
                  for i, debug_file in enumerate(debug_files, start=1):
                      f.write(f"- [Debug Page HTML Page {i}]({debug_file})\n")
                  
                  f.write("\n## URL Details\n")
                  f.write("| No. | Original URL | Google Redirect URL | Domain | VED | USG |\n")
                  f.write("|-----|--------------|---------------------|--------|-----|-----|\n")
                  for i, (_, row) in enumerate(df.iterrows(), start=1):
                      f.write(f"| {i} | {row['original_url']} | {row['google_redirect_url']} | {row['domain']} | {row['ved']} | {row['usg']} |\n")

              print(f"Total Indexed Pages: {total_results}")
              print(f"Number of URLs found: {len(merged_results)}")
              print(f"Results saved to: {filename}")

          if __name__ == "__main__":
              main()
          EOF

      - name: Commit and Push Results
        env:
          GH_PAT: ${{ secrets.GH_PAT }}
        run: |
          git config --global user.name 'github-actions[bot]'
          git config --global user.email 'github-actions[bot]@users.noreply.github.com'
          git add .md/serp-status-*.md
          git add .debug
          git commit -m 'Update Monthly Google Indexing Status' || echo "No changes to commit"
          git push https://x-access-token:${{ secrets.GH_PAT }}@github.com/${{ github.repository }} HEAD:main